<!DOCTYPE html>
<html lang="en">
    <head>
        <meta charset="utf-8" />
        <meta name="viewport" content="width=device-width, initial-scale=1, shrink-to-fit=no" />
        <meta name="description" content="" />
        <meta name="author" content="" />
        <title>Sheila Teo</title>
        <!-- Favicon-->
        <link rel="icon" type="image/x-icon" href="assets/icons/favicon.ico" />    
        <!-- Font Awesome icons (free version)-->
        <script src="https://use.fontawesome.com/releases/v6.1.0/js/all.js" crossorigin="anonymous"></script>
        <!-- Google fonts-->
        <link href="https://fonts.googleapis.com/css?family=Montserrat:400,700" rel="stylesheet" type="text/css" />
        <link href="https://fonts.googleapis.com/css?family=Roboto+Slab:400,100,300,700" rel="stylesheet" type="text/css" />
        <!-- Core theme CSS (includes Bootstrap)-->
        <link href="css/styles.css" rel="stylesheet" />
    </head>
    <body id="page-top">
        <!-- Navigation-->
        <nav class="navbar navbar-expand-lg navbar-dark fixed-top" id="mainNav">
            <div class="container">
                <a class="navbar-brand" href="#page-top"></a>  
                <button class="navbar-toggler" type="button" data-bs-toggle="collapse" data-bs-target="#navbarResponsive" aria-controls="navbarResponsive" aria-expanded="false" aria-label="Toggle navigation">
                    Menu
                    <i class="fas fa-bars ms-1"></i>
                </button>
                <div class="collapse navbar-collapse" id="navbarResponsive">
                    <ul class="navbar-nav text-uppercase ms-auto py-4 py-lg-0">
                        <li class="nav-item"><a class="nav-link" href="#about">About Me</a></li>
                        <li class="nav-item"><a class="nav-link" href="#skills">Skills</a></li>
                        <li class="nav-item"><a class="nav-link" href="#portfolio">Portfolio</a></li>
                        <li class="nav-item"><a class="nav-link" href="#contact">Contact</a></li>
                    </ul>
                </div>
            </div>
        </nav>
        <!-- Masthead-->
        <header class="masthead">
            <div class="container">
                <div class="masthead-subheading">Hello!</div>
                <div class="masthead-heading text-uppercase">My name is Sheila</div>
                <a class="btn btn-primary btn-xl text-uppercase" href="#about">Ready for Takeoff</a>
            </div>
        </header>
        <!-- About-->
        <section class="page-section" id="about">
            <div class="container">
                <div class="text-center">
                    <h2 class="section-heading text-uppercase">About Me</h2>
                    <h3 class="section-subheading text-muted">The flight path that led me to Data Science.</h3>
                </div>
                <ul class="timeline">
                    <li>
                        <div class="timeline-image"><img class="rounded-circle img-fluid" src="assets/images/about/lse.jpg" alt="..." /></div>
                        <div class="timeline-panel">
                            <div class="timeline-heading">
                                <h4>2017</h4>
                                <h4 class="subheading">Economist-in-Training</h4>
                            </div>
                            <div class="timeline-body"><p class="text-muted">I had a favorite subject in high school- Economics. So with all the vigor only an 18-year-old could have, I bundle off to the London School of Economics and Political Science (LSE) to pursue BSc Economics.</p></div>
                        </div>
                    </li>
                    <li class="timeline-inverted">
                        <div class="timeline-image"><img class="rounded-circle img-fluid" src="assets/images/about/laptop.jpg" alt="..." /></div>
                        <div class="timeline-panel">
                            <div class="timeline-heading">
                                <h4>2018</h4>
                                <h4 class="subheading">The Very First Time</h4>
                            </div>
                            <div class="timeline-body"><p class="text-muted">Outside of academics, I join 180 Degrees Consulting, the world's largest consultancy for non-profit organizations, working under British politician Dr. Syed Kamall in using data analytics to devise optimal business strategies for start-ups. My interest in data is piqued.</p></div>
                        </div>
                    </li>
                    <li>
                        <div class="timeline-image"><img class="rounded-circle img-fluid" src="assets/images/about/peking.jpg" alt="..." /></div>
                        <div class="timeline-panel">
                            <div class="timeline-heading">
                                <h4>2019</h4>
                                <h4 class="subheading">Summer Love is Forever</h4>
                            </div>
                            <div class="timeline-body"><p class="text-muted">In my penultimate year of undergraduate studies, I decide to spend a summer at Peking University, pursuing a course in Big Data Analytics for Businesses. I fall madly in love with the field.</p></div>
                        </div>
                    </li>
                    <li class="timeline-inverted">
                        <div class="timeline-image"><img class="rounded-circle img-fluid" src="assets/images/about/columbia.jpg" alt="..." /></div>
                        <div class="timeline-panel">
                            <div class="timeline-heading">
                                <h4>2020</h4>
                                <h4 class="subheading">School Again?!</h4>
                            </div>
                            <div class="timeline-body"><p class="text-muted">Armed with a newfound passion, I pursue MSc Business Analytics (Data Science) at Columbia University, which focuses on applying data science to solve critical business challenges. Concurrently, I make the best memories and friendships in New York City that I'll treasure for a lifetime.</p></div>
                        </div>
                    </li>
                </ul>
            </div>
        </section>
        <!-- Skills -->
        <section class="page-section" id="skills">
            <div class="container">
                <div class="text-center">
                    <h2 class="section-heading text-uppercase">Skills</h2>
                    
                    <left>
                        <pre class="s-code-block language-python">
                            <code>
                                if skills in ['Python', 
                                              'R', 
                                              'SQL', 
                                              'Spark', 
                                              'Scala', 
                                              'Tableau', 
                                              'Power BI', 
                                              'Git', 
                                              'Google Cloud', 
                                              'Microsoft Azure']: 
                                    knowledge = True 
                                
                                else: 
                                    learns_fast = True
                            </code>
                        </pre>
                    </left>
                </div>
            </div>
        </section>
        <!-- Portfolio-->
        <section class="page-section bg-light" id="portfolio">
            <div class="container">
                <div class="text-center">
                    <h2 class="section-heading text-uppercase">My Projects</h2>
                    <h3 class="section-subheading text-muted">Fasten your seat belts, we're about to hit some turbulence!</h3>
                </div>
                <div class="row">
                    <div class="col-lg-4 col-sm-6 mb-4">
                        <!-- 1) Student Data Scientist @ Apple -->
                        <div class="portfolio-item">
                            <a class="portfolio-link" data-bs-toggle="modal" href="#portfolioModal1">
                                <div class="portfolio-hover">
                                    <div class="portfolio-hover-content"><i class="fas fa-plus fa-3x"></i></div>
                                </div>
                                <img class="img-fluid" src="assets/images/portfolio/apple/apple.png" alt="..." />
                            </a>
                            <div class="portfolio-caption">
                                <div class="portfolio-caption-heading">Forecasting supply chain costs</div>
                                <div class="portfolio-caption-subheading text-muted">I worked as a Student Data Scientist at Apple.<br><br><br><b>Cost savings of $20 million yearly.</b></div>
                            </div>
                        </div>
                    </div>
                    <div class="col-lg-4 col-sm-6 mb-4">
                        <!-- Portfolio item 2-->
                        <div class="portfolio-item">
                            <a class="portfolio-link" data-bs-toggle="modal" href="#portfolioModal2">
                                <div class="portfolio-hover">
                                    <div class="portfolio-hover-content"><i class="fas fa-plus fa-3x"></i></div>
                                </div>
                                <img class="img-fluid" src="assets/images/portfolio/spotify/spotify.jpeg" alt="..." />
                            </a>
                            <div class="portfolio-caption">
                                <div class="portfolio-caption-heading">Predicting track-skipping</div>
                                <div class="portfolio-caption-subheading text-muted">Competed in Spotify's<br>Sequential Skip Prediction Challenge.<br><br><b>Top 7% out of 1350 participants.</b></div>                         
                            </div>
                        </div>
                    </div>
                    <div class="col-lg-4 col-sm-6 mb-4">
                        <!-- Portfolio item 3-->
                        <div class="portfolio-item">
                            <a class="portfolio-link" data-bs-toggle="modal" href="#portfolioModal3">
                                <div class="portfolio-hover">
                                    <div class="portfolio-hover-content"><i class="fas fa-plus fa-3x"></i></div>
                                </div>
                                <img class="img-fluid" src="assets/images/portfolio/nestle/nestle.jpg" alt="..." />
                            </a>
                            <div class="portfolio-caption">
                                <div class="portfolio-caption-heading">Forecasting product demand</div>
                                <div class="portfolio-caption-subheading text-muted">I interned as a Data Scientist at Nestlé USA.<br><br><b>Increased demand prediction accuracy by 9.4%.<br>Cost savings of $3.4 million.</b></div>
                            </div>
                        </div>
                    </div>
                    <div class="col-lg-4 col-sm-6 mb-4 mb-lg-0">
                        <!-- Portfolio item 4-->
                        <div class="portfolio-item">
                            <a class="portfolio-link" data-bs-toggle="modal" href="#portfolioModal4">
                                <div class="portfolio-hover">
                                    <div class="portfolio-hover-content"><i class="fas fa-plus fa-3x"></i></div>
                                </div>
                                <img class="img-fluid" src="assets/images/portfolio/google/google.jpg" alt="..." />
                            </a>
                            <div class="portfolio-caption">
                                <div class="portfolio-caption-heading">NLP: Scoring Q&A algorithms
                                    </div>
                                <div class="portfolio-caption-subheading text-muted">Competed in Google Research's<br>Question-Answering Labelling Competition.<br><br><b>Top 15% out of 1571 participants.</b></div>
                            </div>
                        </div>
                    </div>
                    <div class="col-lg-4 col-sm-6 mb-4 mb-sm-0">
                        <!-- Portfolio item 5-->
                        <div class="portfolio-item">
                            <a class="portfolio-link" data-bs-toggle="modal" href="#portfolioModal5">
                                <div class="portfolio-hover">
                                    <div class="portfolio-hover-content"><i class="fas fa-plus fa-3x"></i></div>
                                </div>
                                <img class="img-fluid" src="assets/images/portfolio/funnel/funnel.jpeg" alt="..." />
                            </a>
                            <div class="portfolio-caption">
                                <div class="portfolio-caption-heading">Predicting prospect conversion</div>
                                <div class="portfolio-caption-subheading text-muted">I worked as a Student Machine Learning Engineer<br>at Funnel, an AI start-up in NYC.<br><br><b>50% increase in successful prospect conversions.</b></div>
                            </div>
                        </div>
                    </div>
                    <div class="col-lg-4 col-sm-6">
                        <!-- Portfolio item 6-->
                        <div class="portfolio-item">
                            <a class="portfolio-link" data-bs-toggle="modal" href="#portfolioModal6">
                                <div class="portfolio-hover">
                                    <div class="portfolio-hover-content"><i class="fas fa-plus fa-3x"></i></div>
                                </div>
                                <img class="img-fluid" src="assets/images/portfolio/columbia/columbia.png" alt="..." />
                            </a>
                            <div class="portfolio-caption">
                                <div class="portfolio-caption-heading">Predicting click-through rates</div>
                                <div class="portfolio-caption-subheading text-muted">Competed in Columbia Engineering<br>competition open to Masters students.<br><br><b>Placed 4<sup>th</sup> out of 200 participants.</b></div>
                            </div>
                        </div>
                    </div>
                </div>
            </div>
        </section>
        <!-- Contact-->
        <section class="page-section" id="contact">
            <div class="container">
                <div class="text-center">
                    <h2 class="section-heading text-uppercase">Get In Touch!</h2>
                </div>

                <div class="container">
                    <div>
                        <center>
                            <div class="col-lg-4 my-3 my-lg-0">
                                <a href="https://www.linkedin.com/in/sheila-teo/" aria-label="LinkedIn" target="_blank"><i class="fab fa-linkedin-in fa-2xl"></i></a>&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;

                                <a href="https://github.com/sheilateozy" aria-label="GitHub" target="_blank"><i class="fab fa-github fa-2xl"></i></a>&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;
                                <a href="mailto:sheilateozy@gmail.com" aria-label="Email" target="_blank"><i class="fa fa-envelope fa-2xl"></i></a>
                            </div>
                        </center>   
                    </div>
                </div>
            </div>
        </section>

        <!-- Portfolio Pages -->
        <!-- Page 1: Apple -->
        <div class="portfolio-modal modal fade" id="portfolioModal1" tabindex="-1" role="dialog" aria-hidden="true">
            <div class="modal-dialog">
                <div class="modal-content">
                    <div class="close-modal" data-bs-dismiss="modal"><img src="assets/icons/close.svg" alt="Close modal" /></div>
                        <div class="modal-body">
                            <!-- Project details-->
                            <h2 class="text-uppercase">Forecasting Apple's Supply Chain Costs</h2>
                            <h3 class="text-warning">ARIMA</h3>
                            <p class="item-intro text-muted">Desktop application that uses ARIMA models to predict cost and detect errors.<br>Data visualisation dashboarding on Tableau to depict costs across components and time.<br><b>Cost savings of $20 million yearly.</b></p>
                
                            <button class="btn btn-primary btn-xl text-uppercase" type="button">
                                <a href="https://github.com/sheilateozy/Apple_Forecasting-Supply-Chain-Costs" style="text-decoration: none; color: white;" target="_blank">View on GitHub</a>
                            </button>
                            <br>
                            <br>
                            <br>

                            <div class="container">
                                <div class="row justify-content-center">
                                    <div class="col-lg-8" style="text-align:justify";>
                                    <h3 style="text-align:left;">About</h3>
                                        <p>I worked as a Student Data Scientist at Apple, in a team of 5 students from Columbia Engineering.</p>
                                        
                                        <p>Apple purchases components parts for its products from third party suppliers. The prices of these parts can be volatile at times due to trends and seasonalities, which snowballs into significant supply chain cost impacts.<br><br>
                                            Our project uses ARIMA models to predict a reasonable cost range for hundreds of parts in each month. These models work in the back-end of the following desktop application:</p>
                                            <center><img class="img-fluid" src="assets/images/portfolio/apple/desktop_application.png"></center>
                                        <p>The application takes in data on component part costs, in both Excel and CSV formats. Upon running the ARIMA models, it then flags out warnings for component parts whose costs are higher than they should be, and produces analysis reports explaining the likely reasons behind the cost spike.<br><br>
                                            This is used in real-time to catch areas in which Apple may be overpaying in its supply chain, or used on an ex-ante basis to negotiate future part costs with suppliers.</p>
                                    <br>
                                    <h3 style="text-align:left;">Results: Making an impact at Apple</h3>
                                            <p>The ARIMA models have a small average forecasting error of < 0.1% MAPE (Mean Absolute Percentage Error). This allows component part costs to be accurately forecasted, translating into cost savings of $20 million annually.</p>
                                            <center><img class="img-fluid" src="assets/images/portfolio/apple/cost_savings.png"></center>
                                    <br>
                                    <h3 style="text-align:left;">Project Methodology</h3>
                                        <br>
                                        <h4 style="text-align:left;">1. Data simulation</h4>
                                        <p>Remove outliers in data to discard noise. Next, simulate data for component parts with lesser cost history data using a normal distribution with the same mean and variance as the available data.</p>
                                        <center><img class="img-fluid" src="assets/images/portfolio/apple/data_simulation.png"></center>
                                        
                                        <h4 style="text-align:left;">2. Train &amp tune ARIMA hyperparameters using Grid Search</h4>
                                        <p>We build one ARIMA model for each component part's cost. As such, 768 models are built. For each model, grid search for its optimal ARIMA (p, d, q) hyperparameters using k-fold cross-validation, 
                                            where the optimal hyperparameters are determined by the lowest MAPE on a left-out test set.</p>

                                        <h4 style="text-align:left;">3. Make predictions with prediction intervals</h4>
                                        <p>When predicting the cost of a component part for the next month, apart from simply outputting cost predictions, calculate a 95% confidence interval for the prediction. This serves as a reasonable cost range for the part. Below is a graphical illustration, where the cost range is shown as a shaded grey area.</p>
                                        <center><img class="img-fluid" src="assets/images/portfolio/apple/prediction.png"></center>
                                    <br>
                                    <br>
                                    <h3 style="text-align:left;">Still curious?</h3>
                                    <p>My team made a presentation to Apple management, and you can check out our slides below.</p>

                                    <center>
                                        <button class="btn btn-primary btn-xl text-uppercase" type="button">
                                            <a href="https://github.com/sheilateozy/Apple_Forecasting-Supply-Chain-Costs/blob/main/presentation_slides/presentation_slides.pdf" style="text-decoration: none; color: white;" target="_blank">Presentation Slides</a>
                                        </button>

                                        <button class="btn btn-primary btn-xl text-uppercase" data-bs-dismiss="modal" type="button">
                                            <i class="fas fa-xmark me-1"></i>
                                            Close Project
                                        </button>
                                    </center>
                                </div>
                            </div>
                        </div>
                    </div>
                </div>
            </div>
        </div>
        <!-- Page 2: Spotify -->
        <div class="portfolio-modal modal fade" id="portfolioModal2" tabindex="-1" role="dialog" aria-hidden="true">
            <div class="modal-dialog">
                <div class="modal-content">
                    <div class="close-modal" data-bs-dismiss="modal"><img src="assets/icons/close.svg" alt="Close modal" /></div>

                    <div class="modal-body">
                        <!-- Project details-->
                        <h2 class="text-uppercase">Spotify competition:<br>Predicting sequential track-skipping</h2>
                        <h3 class="text-warning">XGBoost</h3>
                        <p class="item-intro text-muted">
                            Windowed feature engineering for sequential information.<br>
                            Regularized k-fold target encoding of categorical features.<br>
                            Hyperparameter tuning using Bayesian Optimization.<br>
                            Threshold-moving to minimize expected costs in real-world model deployment.<br>
                            <b>Top 7% out of 1350 participants.</b></p>

                        <button class="btn btn-primary btn-xl text-uppercase" type="button">
                            <a href="https://github.com/sheilateozy/Spotify_Predicting-Track-Skipping" style="text-decoration: none; color: white;" target="_blank">View on GitHub</a>
                        </button>
                        <br>
                        <br>
                        <br>

                        <div class="container">
                            <div class="row justify-content-center">
                                <div class="col-lg-8">
                                    <h3 style="text-align:left;">About</h3>
                                    <p style="text-align:justify;">A central challenge for Spotify is to recommend the right music to each user. While there exists a large body of work on recommender systems, there is at present little work describing how users sequentially interact with the streamed content they are presented
                                    with. Music content is unique in that the question of if, and when, a user skips a track is an important implicit feedback signal on the quality of the system's recommendation.
                                    <br><br>
                                    In the Spotify Sequential Skip Prediction Challenge, I explore this important and understudied problem in music streaming by building a model that predicts whether a user will skip or listen to tracks that they are streamed, given their immediately preceding interactions in a listening session. The model's accuracy score of 76.8% places in the top 7% on the challenge leaderboard, out of 1350 participants.</p>
                                    <br>

                                    <h3 style="text-align:left;">Project Methodology</h3>
                                        <br>
                                        <h4 style="text-align:left;">1. Feature engineering to capture sequential information</h4>
                                        <p style="text-align:justify;">Since the aim of this paper is to predict sequential track-skipping behaviour, it is crucial to engineer
                                        smart features that are able to capture the sequential nature of our data. Some of the features engineered for this include:</p>
                                            <li style="text-align:left;"><i>skip_previous</i>: Binary variable that depicts if the track encountered right before the current one was skipped by the user.</li>
                                            <br>
                                            <li style="text-align:left;"><i>skip_prop_prior_to_track</i>: Depicts the proportion of skips that the user has made on all tracks encountered prior to the current track.</li>
                                            <br>
                                            <li style="text-align:justify;"><i>skip_prop_prior_to_track_SD</i>: Depicts the
                                                standard deviation of <i>skip_prop_prior_to_track</i> at each point in time.
                                                This depicts the consistency of the user’s skipping action prior to the current track. 
                                                For instance, <i>skip_prop_prior_to_track_SD=0</i> on track 5 implies that the user had made the same action on all of
                                                the 4 prior tracks (either skipped all or not skipped all). We can therefore be more certain of the
                                                predictive power of <i>skip_prop_prior_to_track</i> for forecasting user action on this 5th track, since the
                                                user is very likely to also make the same action. As such, the lower the <i>skip_prop_prior_to_track_SD</i>,
                                                the more consistent the user’s past track-skipping behavior, and the more certain we are in predicting
                                                user action on the next track.</li>
                                        <br>
                                        <p style="text-align:justify;">In particular, <i>skip_previous</i> and <i>skip_prop_prior_to_track</i>
                                            are found to be among the most important predictors of track-skipping behavior:</p>
                                            <center><img class="img-fluid" src="assets/images/portfolio/spotify/feature_importance.png"></center>

                                        
                                        <h4 style="text-align:left;">2. Encode categorical features: Regularized k-fold target encoding</h4>
                                            <p style="text-align:justify;">
                                                Target Encoding is the substitution of a category with the mean target value for that category.
                                                However, this introduces target leakage since the target-encoded value for an observation includes the effect of that observation's target value,
                                                thus this encoded feature value for the observation has the target baked inside it. 
                                            </p>

                                            <p style="text-align:justify;">
                                                K-Fold Target Encoding builds upon traditional Target Encoding by alleviating this issue. 
                                                It ensures that the target value of an observation is not used to compute its target-encoded predictors. 
                                                The dataset is first split into <i>k</i> folds, and the mean target value for each category in the <i>i<sup>th</sup></i> fold is computed using data in the other <i>k-1</i> folds. 
                                            </p>

                                            <p style="text-align:justify;">
                                                The regularized aspect next comes into play to further prevent overfitting. 
                                                In order to provide regularization, I additionally add random noise into the target-encoded values for each category.
                                            </p>

                                        <h4 style="text-align:left;">3. Select metric of focus: F2-Score</h4>
                                            <p style="text-align:justify;">The metrics of particular significance in our specific business case are:</p>
                                                <li style="text-align:left;">Recall: It is of greater business utility to identify tracks that will be skipped over ones that will
                                                    not. Identification of these songs can pave the way for Spotify to improve on their track
                                                    recommendation system.</li>
                                                <br>
                                                <li style="text-align:left;">Precision: It is important for the model to be certain when identifying a track that will be skipped.</li>
                                                <br>
                                                <li style="text-align:left;">F2-Score: Given our combined emphasis on both Recall and Precision, I ultimately look at the
                                            F2-Score as a combined measure of both. In particular, choosing the F2-Score over the more
                                            commonly utilized F1-Score implies Recall is weighed twice as heavily as compared to Precision, as opposed to equally.
                                            This follows from my assumption made above that a FN generates twice the cost of a FP for
                                            Spotify in real-world model deployment.</li>
                                        
                                        <br><br>

                                         <h4 style="text-align:left;">4. Train &amp tune XGBoost hyperparameters using Bayesian Optimization</h4>
                                            <p style="text-align:justify;">I utilize a Bayesian Optimization search method with stratified k-fold cross validation in order to obtain the
                                            optimal hyperparameters in the most efficient manner. 
                                            <br><br>This is as compared to using simpler
                                            hyperparameter search methods such as Grid Search and Random Search, where each
                                            hyperparameter combination searched during tuning is independent of the last. In contrast, Bayesian
                                            Optimization uses knowledge of previous iterations of the algorithm such that each new search is
                                            guided from previous search results. This allows the algorithm to obtain optimal hyperparameters in
                                            as few iterations as possible, thus reducing computational cost.
                                         </p>
                                         <br>

                                    <h3 style="text-align:left;">One cool takeaway:<br>Threshold-moving for real-world model deployment</h3>
                                        <p style="text-align:justify;">As opposed to simply using the default decision threshold of 0.5 to classify a positive condition, threshold-moving is important in the real-world deployment of models.
                                            The default threshold of 0.5 will only be optimal if the business costs of a False Negative (FN) and a False Positive (FP) are equal. However, this is rarely the case in reality.
                                            <br><br>In this project, an FP occurs when a song is identified to be skipped and thus not recommended to a user, but in
                                            fact would not have been skipped. Hence, costs are incurred due to the user not being
                                            exposed to a song he would have enjoyed. However, this presents a relatively lower cost to
                                            Spotify since user satisfaction is unaffected. 
                                            <br><br>On the other hand, a FN occurs when a song identified to not be skipped is in fact skipped by a
                                            user. Costs are incurred due to lower user satisfaction with the streaming service. This presents a
                                            much higher cost to Spotify. As such, the costs of FP and FN are unequal.
                                            <br><br>I assume that a FN generates twice the cost of a FP for
                                            Spotify. For each decision threshold, I obtain the resulting confusion matrix and subsequently
                                            compute expected costs as 2𝐹𝑁 + 𝐹𝑃, resulting in the below graph:</p>
                                        <br>
                                        <center><img class="img-fluid" src="assets/images/portfolio/spotify/threshold_moving.png"></center>
                                        <br>
                                        <p style="text-align:justify;">The optimal threshold is the one resulting in
                                            the minimum expected cost. This optimal threshold is found to be 0.2655, ie. a track is classified to be skipped as long
                                            as its predicted probability from the final XGBoost model is above 0.2655. </p>
                                    <br>

                                    <h3 style="text-align:left;">Still curious?</h3>
                                    <p style="text-align:justify;">I authored a research paper covering the details of my methodology and findings when I took a PhD-level Machine Learning class at Columbia. This paper was awarded the top mark in the class (full-mark grade).</p>
                                    
                                    <center>
                                        <button class="btn btn-primary btn-xl text-uppercase" type="button">
                                            <a href="https://github.com/sheilateozy/Spotify_Predicting-Track-Skipping/blob/main/research_paper/research_paper.pdf" style="text-decoration: none; color: white;" target="_blank">Read the paper</a>
                                        </button>

                                        <button class="btn btn-primary btn-xl text-uppercase" data-bs-dismiss="modal" type="button">
                                            <i class="fas fa-xmark me-1"></i>
                                            Close Project
                                        </button>
                                    </center>
                                </div>
                            </div>
                        </div>
                    </div>
                </div>
            </div>
        </div>

        <!-- Page 3: Nestle -->
        <div class="portfolio-modal modal fade" id="portfolioModal3" tabindex="-1" role="dialog" aria-hidden="true">
            <div class="modal-dialog">
                <div class="modal-content">
                    <div class="close-modal" data-bs-dismiss="modal"><img src="assets/icons/close.svg" alt="Close modal" /></div>
                    <div class="modal-body">
                        <!-- Project details-->
                        <h2 class="text-uppercase">Forecasting Nestlé's product demand</h2>
                        <h3 class="text-warning">ARIMAX, Elastic Net, Random Forest, XGBoost, Deep Neural Network</h3>
                        <p class="item-intro text-muted">
                            SQL to query and manipulate big data on SAS.<br>
                            Designed, implemented, and back-tested optimal models using custom scoring metrics.<br>
                            Feature selection using Recursive Feature Elimination.<br>
                            Data visualisation dashboarding on Microsoft Power BI to depict business impact of new models.<br>
                            <b>Increased demand prediction accuracy by 9.4% from existing models.</b></p>

                            <button class="btn btn-primary btn-xl text-uppercase" type="button">
                                <a href="https://github.com/sheilateozy/Nestle-_Forecasting-Product-Demand" style="text-decoration: none; color: white;" target="_blank">View on GitHub</a>
                            </button>
                            <br>
                            <br>
                            <br>

                            <div class="container">
                                <div class="row justify-content-center">
                                    <div class="col-lg-8" style="text-align:justify;">
                                        <h3 style="text-align:left;">About</h3>
                                            <p>I interned as a Data Scientist at Nestlé USA on the Supply Chain Analytics team, which uses Data Science to forecast demand for hundreds of thousands of Nestlé’s products. My work involved improving their existing demand forecasting models through 3 aspects, as follows:</p>
                                            <br>
                                            <h4 style="text-align:left;">1. Feature selection tool: Recursive Feature Elimination</h4>
                                                <p>Nestlé collects an enormous amount of data, such as demand data, promotional data, holiday data, and competitor data. With this large amount of features available to be used for modelling, the team required a robust feature selection tool. 
                                                    <br><br>The first aspect of my project involved designing an end-to-end framework for feature selection (code and documentation).
                                                    The goal was to (i) Increase model accuracy through technical feature selection methods instead of human intuition and domain expertise, and (ii) Increase model explainability to non-technical stakeholders within Nestlé.</p>
                                                <p>I designed an approach that utilized Recursive Feature Elimination (RFE) through Permutation Feature Importance with Random Forests. This new approach improves on the current feature selection approach used by the team through the following aspects:</p>
                                                <center><img class="img-fluid" src="assets/images/portfolio/nestle/feature_selection_tool.png"></center>


                                            <br>
                                            <h4 style="text-align:left;">2. Feature selection at different levels</h4>
                                                    <p>Next, I explored different levels in which to conduct feature selection. Feature selection can be conducted at 3 main levels:</p>
                                                        <li style="text-align:justify;"><b>Business level:</b> The term “business” at Nestlé refers to a category of products that Nestlé sells, examples being the Baking business, the Beverage business, and so on. Under this level, feature selection is conducted for an entire business of products, and this same feature subset is used to forecast demand for all products under that business.</li>
                                                        <br>
                                                        <li style="text-align:justify;"><b>Product level:</b> Within any one business, there are hundreds of products. For example, two different products within Nestlé’s Baking business are pies and cakes. Under this level, feature selection is conducted for each unique product, thus different feature subsets are used to forecast demand for different products within the same business.</li>
                                                        <br>
                                                        <li style="text-align:justify;"><b>Product and store level:</b> Nestlé sells its products at thousands of stores around the world. Different products are sold at each store. Under this level, feature selection is conducted for each unique product and store pair, thus different feature subsets are used to forecast demand for each product-store pair.</li>
                                                        <br>
                                                        <center><img class="img-fluid" src="assets/images/portfolio/nestle/feature_selection_business.png"></center>
                                                        <center><img class="img-fluid" src="assets/images/portfolio/nestle/feature_selection_product.png"></center>
                                                        <center><img class="img-fluid" src="assets/images/portfolio/nestle/feature_selection_product-store.png"></center>

                                                    <p>Among the 3 levels, I determine the optimal level at which feature selection should be conducted by back-testing across various time periods to find the level that results in the highest forecasting accuracy.</p>

                                            <h4 style="text-align:left;">3. Benchmark suite of machine learning models:<br>Elastic Net, Random Forest, XGBoost, Deep Neural Network</h4>
                                            <p>The team had primarily been using traditional Statistics-based models such as ARIMA, ARIMAX, and other regression-based models for time-series demand forecasting. 
                                                However, they were keen to branch out and adopt other machine learning-based models. Hence, I also worked on benchmarking models that had not yet been used by the team prior, using a custom scoring metric termed Demand Planning Accurcacy at Nestlé.
                                        
                                        <br>
                                        <br>
                                        <h3 style="text-align:left;">Results: Making an impact at Nestlé</h3>
                                            <p>My work increased the accuracy of product demand forecasts by 9.4% on average across 6 categories of Nestlé products, with the exact breakdown as follows:</p>
                                            <center><img class="img-fluid" src="assets/images/portfolio/nestle/accuracy_impact.png"></center>

                                            <p>This translates into cost savings of $3.4 million across the various categories:</p>
                                            <center><img class="img-fluid" src="assets/images/portfolio/nestle/cost_savings.png"></center>
                                        <br>
                                        <br>
                                    
                                        <h3 style="text-align:left;">Project Methodology</h3>
                                            <br>
                                            <h4 style="text-align:left;">1. Feature engineering: To extract time nature of data</h4>
                                                    <li style="text-align:left;">Lag variables of past demand and consumption information</li>
                                                    <li style="text-align:left;">Rolling mean and rolling standard deviation of existing features</li>
                                                    <li style="text-align:left;">Calendar signatures, eg. Week number in year</li>
                                            <br>
                                            <h4 style="text-align:left;">2. Encode categorical variables using Hash Encoding</h4>
                                                <p>The categorical variables in the dataset are of high cardinality. As such, I utilize Hash Encoding, which reduces the number of new features created from the encoding to a lower dimension.
                                                    This dimension is chosen from a trade-off: The larger the number of dimensions, the more information we retain from the original categorical feature after encoding it. However, this also results in sparser feature matrices that may reduce the performance of any machine learning model.</p>

                                            <h4 style="text-align:left;">3. Train &amp tune Random Forest hyperparameters using Random Search</h4>
                                                <p>I utilize a Random Search method with 5-fold cross validation, conducted in parallel with Apache Spark backend for faster runtime.</p>

                                            <h4 style="text-align:left;">4. Obtain unbiased permutation feature importances</h4>
                                                <p>Taking extra care to group highly collinear features together to obtain unbiased importances.</p>

                                            <h4 style="text-align:left;">5. Feature selection using Recursive Feature Elimination (RFE)</h4>
                                                <p>RFE is a recursive process in that it progressively considers smaller and smaller subsets of the original features. 
                                                    The process starts by using all the original features, building a Random Forest with them, and obtaining feature importance scores for all the features. This describes steps 1 and 2 outlined above.
                                                    Next, we discard the feature identified to be the least important, and repeat the above. 
                                                    This repetition continues until the size of the feature set has been incrementally reduced by 1 until it becomes 0. 
                                                    Following which, the optimal feature subset is the one that produces the highest model accuracy score on the validation set.</p>

                                            <h4 style="text-align:left;">6. Train model on optimal subset of features</h4>
                                                <p>Obtain test set predictions for various time periods to validate model</p>

                                            <h4 style="text-align:left;">7. Repeat for various levels of forecasting to determine optimal level</h4>
                                                <p>As outlined above, there are 3 main levels of forecasting investigated: At the business level, product level, and product-store level.</p>
                                                
                                            <h4 style="text-align:left;">8. Repeat for different models to determine optimal model</h4>
                                                <p>Machine learning is an iterative process. I explore the following models to determine the best for this project's use-case: Elastic Net, Random Forest, XGBoost, Deep Neural Network. </p>
                                    <br>

                                    <h3 style="text-align:left;">One cool takeaway:<br>Reducing bias in permutation feature importance in the presence of highly collinear features</h3>
                                        <p>Permutation feature importance is touted as the gold-standard method for obtaining true feature importances. This is as opposed to other methods such as impurity-based feature importances which tend to inflate the importance of continuous or high-cardinality categorical variables (Read more about this here! <a href="https://explained.ai/rf-importance/index.html#3">Beware Default Random Forest Importances</a>).
                                            It works by randomly shuffling (permutating) each feature such that the relationship between that feature and the target is broken. With this relationship now broken, we use the same model to predict the target. The importance of the feature is then measured as the fall in accuracy of the model from before and after it was permuted.</p>

                                        <p>However, trouble sets in when there exists highly collinear features in the data. In this case, permutating one feature will have little effect on the models performance because it can get the same information from a correlated feature. This leads to the traditional permutation feature importance method understating the importance of such features.</p>
                                    
                                        <p>To overcome this problem, I group features that are collinear and permute them together as a meta-feature. This allows me to obtain the true unbiased importance of highly collinear features as a group.</p>
                                    <br>

                                    <h3 style="text-align:left;">Still curious?</h3>
                                        <p>I authored an internship report covering the details of my methodology, and also made a presentation to Nestlé management.</p>

                                    <center>
                                        <button class="btn btn-primary btn-xl text-uppercase" type="button">
                                            <a href="https://github.com/sheilateozy/Nestle-_Forecasting-Product-Demand/blob/main/internship_report/internship_report.pdf" style="text-decoration: none; color: white;" target="_blank">Read the report</a>
                                        </button>

                                        <button class="btn btn-primary btn-xl text-uppercase" type="button">
                                            <a href="https://github.com/sheilateozy/Nestle-_Forecasting-Product-Demand/blob/main/presentation_slides/presentation_slides.pdf" style="text-decoration: none; color: white;" target="_blank">Presentation slides</a>
                                        </button>

                                        <button class="btn btn-primary btn-xl text-uppercase" data-bs-dismiss="modal" type="button">
                                            <i class="fas fa-xmark me-1"></i>
                                            Close Project
                                        </button>
                                    </center>

                                </div>
                            </div>
                        </div>
                    </div>
                </div>
            </div>
        </div>
        <!-- Page 4: Google -->
        <div class="portfolio-modal modal fade" id="portfolioModal4" tabindex="-1" role="dialog" aria-hidden="true">
            <div class="modal-dialog">
                <div class="modal-content">
                    <div class="close-modal" data-bs-dismiss="modal"><img src="assets/icons/close.svg" alt="Close modal" /></div>
                                <div class="modal-body">
                                    <!-- Project details-->
                                    <h2 class="text-uppercase">NLP: Scoring Q&A algorithms</h2>
                                    <h3 class="text-warning">Bi-Directional LSTM with Convolutional Layers</h3>
                                    <p class="item-intro text-muted">
                                        Deep learning using Keras, PyTorch.<br>
                                        Multi-target task: Predict 30 targets using 1 model.<br>
                                        Text embeddings extraction using GloVE, BERT.<br>
                                        <b>Top 15% out of 1571 participants.</b></p>


                                    <button class="btn btn-primary btn-xl text-uppercase" type="button">
                                        <a href="https://github.com/sheilateozy/Google_Scoring-QA-Algorithms" style="text-decoration: none; color: white;" target="_blank">View on GitHub</a>
                                    </button>
                                    <br>
                                    <br>
                                    <br>
        
                                    <div class="container">
                                        <div class="row justify-content-center">
                                            <div class="col-lg-8" style="text-align:justify;">
                                                <h3 style="text-align:left;">About</h3>
                                                    <p>Question-answering is a discipline within the field of natural language processing that is concerned with building systems that automatically answer questions posed by humans in a natural language.</p>

                                                    <p>This project builds a deep learning model that predicts the quality of automated answers to human-posed questions, taking only text features of the question title, question body, and answer.</p>
                                                <br>  
                                                <br>
                                                <h3 style="text-align:left;">Project Methodology</h3>
                                                <br>
                                                    <h4 style="text-align:left;">1. Preprocess text features</h4>
                                                        <p> 
                                                        ▪ Convert all text to lowercase<br>
                                                        ▪ Separate joined words commonly used in speech. For instance, “I’m” is separated into “I am”<br>
                                                        ▪ Remove punctuation<br>
                                                        ▪ Remove stop words 
                                                        </p>

                                                    <h4 style="text-align:left;">2. Create numeric representations for text features</h4>
                                                        <p>One of the main design decisions when building a deep learning model with text data is the question of whether to utilize pre-trained word embeddings or to construct an embedding layer within the model to be trained from scratch with respect to the problem at hand.
                                                        Since the dataset available for this task is relatively small with 10,000 rows, I choose to employ the transfer learning method instead of training my own.
                                                        I explore 2 state-of-the-art pre-trained algorithms for this: GloVe and BERT.</p>

                                                        <h5 style="text-align:left;">2.1. GloVE (Global Vectors for Word Representation)</h4>
                                                            <p>GloVE is a pre-trained word embedding algorithm trained on Wikipedia and Gigaword data. The advantage of GloVe lies in its utilization of both local statistics (local context information of words) and global statistics (word co-occurrence) to obtain word embeddings.</p>
                                                            <p>I first tokenize and pad all text using Keras’ Tokenizer class. Within the subsequent model architecture, I replace the parameters of the embedding layer with these pre-trained embeddings and freeze the layer, preventing its weights from being updated during model training.
                                                                Importantly, since the test set must remain unseen to avoid introducing bias, I utilize the same tokenizer trained on the training set in order to tokenize words in the test set. This implies that words found only in the test set and not the training set cannot be embedded, and have their embeddings set to 0. I also pad to the same length and specifications as the training set.</p>

                                                        <h5 style="text-align:left;">2.2. BERT (Bidirectional Encoder Representations from Transformer)</h4>
                                                            <p>BERT is a pre-trained language model created at Google that applies the bidirectional training of transformers to language modelling, allowing for a deeper sense of language context. Traditional single-direction language models produce a fixed representation for a word regardless of the context within which the word appears. In contrast, BERT’s representations for words are dynamically informed by the words around them.</p>
                                                            <p>I follow a strategy proposed by BERT’s original creators: I first pass the dataset’s text data into the pre-trained BERT model and obtain the weights on its last 4 hidden layers. The summation of these weights is then used as input features for the subsequent deep learning model. These were conducted using PyTorch with the Hugging Face library.</p>

                                                    <h4 style="text-align:left;">3. Model architecture: LSTM with Convolutional layers</h4>
                                                        <p>There exists many different model architectures possible for this prediction task. In order to determine the optimal architecture, I took inspiration from the winners of similar competitions on Kaggle. In particular, I experiment with the model architecture used by the winner of a recent NLP competition held by Quora:</p>
                                                        <center><img class="img-fluid" src="assets/images/portfolio/google/model_architecture.png"></center>

                                                        <p>Importantly, I choose a combination of both convolutional and LSTM layers, an increasingly popular model architecture in text modelling. The presence of both types of layers builds a network that is able to utilize both spatial and temporal information. Specifically, the convolutional layer extracts spatially-aware features from word embedding inputs and the subsequent LSTM layer interprets these features across time.</p>

                                                    <h4 style="text-align:left;">4. Model tuning: Increase model regularization</h4>
                                                        <p>
                                                            I carry out model tuning using a left-out test set.
                                                            ▪ Increase traditional dropout<br>
                                                            ▪ Add recurrent dropout to LSTM layer in order for recurrent weights to be additionally regularized
                                                        </p>

                                                    <h4 style="text-align:left;">5. Model tuning: Increase model complexity</h4>
                                                        <p>
                                                        ▪ Increase number of convolutional layers from 1 to 2<br>
                                                        ▪ Increase number of LSTM layers from 1 to 3<br>
                                                        ▪ Tune learning rate used under the Adam optimizer to determine an optimal value
                                                        </p>
                                                <br>

                                                <h3 style="text-align:left;">One cool takeaway:<br>Selecting optimal loss function under multi-target task</h3>
                                                    <p>For each question-answer pair, the 30 targets to be predicted are the quality of the question and answer along 30 different metrics, as follows:</p>
                                                    <center><img class="img-fluid" src="assets/images/portfolio/google/targets.png"></center>

                                                    <p>At first glance, since the 30 targets are each scored between 0 and 1, the immediate impression is that all 30 targets are continuous. However, investigating the targets using a boxplot reveals the following:</p>
                                                    <center><img class="img-fluid" src="assets/images/portfolio/google/targets_boxplot.png"></center>
                                                    <p>10 of these targets take on less than 10 unique values each. This indicates that these 10 targets should instead be regarded as categorical instead of continuous, contrary to the initial impression that all 30 targets are continuous.</p>
                                                    
                                                    <p>In light of this, I implement the binary cross-entropy loss for my model, due to:
                                                        <li>As mentioned, 10 of the 30 targets should instead be regarded as categorical instead of continuous. This lends strength towards utilizing binary cross-entropy loss instead of regression-based losses.</li>
                                                        <br>
                                                        <li>More importantly though, all the targets have a value of between 0 and 1. As such, they can be treated akin to probabilities under categorical target prediction. The usage of binary cross-entropy loss ensures that predictions for these target values remain between the range of 0 and 1, as compared to using traditional regression-based losses which will not.</li></p>
                                                <br>
                                                <br>
                                                <h3 style="text-align:left;">Still curious?</h3>
                                                <p>I authored a research paper covering the details of my methodology and findings when I took a PhD-level Deep Learning class at Columbia.</p>
        
                                            <center>
                                                <button class="btn btn-primary btn-xl text-uppercase" type="button">
                                                    <a href="https://github.com/sheilateozy/Google_Scoring-QA-Algorithms/blob/main/research_paper/research_paper.pdf" style="text-decoration: none; color: white;" target="_blank">Read the paper</a>
                                                </button>
        
                                                <button class="btn btn-primary btn-xl text-uppercase" data-bs-dismiss="modal" type="button">
                                                    <i class="fas fa-xmark me-1"></i>
                                                    Close Project
                                                </button>
                                            </center>
                                </div>
                            </div>
                        </div>
                    </div>
                </div>
            </div>
        </div>
        <!-- Page 5: Funnel-->
        <div class="portfolio-modal modal fade" id="portfolioModal5" tabindex="-1" role="dialog" aria-hidden="true">
            <div class="modal-dialog">
                <div class="modal-content">
                    <div class="close-modal" data-bs-dismiss="modal"><img src="assets/icons/close.svg" alt="Close modal" /></div>
                                <div class="modal-body">
                                    <!-- Project details-->
                                    <h2 class="text-uppercase">Predicting Funnel's prospect conversion</h2>
                                    <h3 class="text-warning">Logistic Regression, CatBoost, XGBoost</h3>
                                    <p class="item-intro text-muted">
                                        Unique modelling approach of building 1 model per stage of prospect journey.<br>
                                        Extensive feature engineering from working with business stakeholders to obtain domain knowledge.<br>
                                        Tailored machine learning metrics to business goals.<br>
                                        Black-box model explanation using SHAP (SHapley Additive exPlanations).<br>
                                        <b>50% increase in successful prospect conversions.</b>
                                    </p>


                                    <button class="btn btn-primary btn-xl text-uppercase" type="button">
                                        <a href="https://github.com/sheilateozy/Funnel_Predicting-Prospect-Conversion" style="text-decoration: none; color: white;" target="_blank">View on GitHub</a>
                                    </button>
                                    <br>
                                    <br>
                                    <br>
        
                                    <div class="container">
                                        <div class="row justify-content-center">
                                            <div class="col-lg-8" style="text-align:justify;">
                                                <h3 style="text-align:left;">About</h3>
                                                    <p>I worked as a Student Machine Learning Engineer at Funnel, as part of a team of 5.
                                                        Our team consisted of 2 students from Columbia Engineering and 3 MBA students from Columbia Business School, coming together to tackle a real-world business problem at Funnel.
                                                    </p>

                                                    <p>
                                                        Funnel provides property management software whose target group are leasing agents. 
                                                        Leasing agents use their software to keep track of prospective renters who express interest in the properties they manage.
                                                    </p>

                                                    <p>Our project involves creating a machine learning model that scores prospective renters in terms of their conversion rate, ie. probability of eventually renting from the leasing agent.</p>
                                                        
                                                    <p>Prior to this project, leasing agents who use Funnel's software have no predetermined method of filtering through hundreds of new prospective renters that come in everyday, in deciding which ones are more important to follow up on.
                                                        This project aims to enhance the software's capabilities by integrating this machine learning model into its system. With this, all prospective renters that express interest will be ranked by predicted conversion rate and leasing agents
                                                        can then prioritize them accordingly to help maximise conversion results. This increases the utility that Funnel's software provides to leasing agents.</p>
                                                <br>
                                                <br>
                                                <h3 style="text-align:left;">Results: Making an impact at Funnel</h3>
                                                    <p>In total, we built 3 models that each predicts a prospective renter's conversion rate at a distinct point of their journey.
                                                        The models have an ROC-AUC of 91%, 60%, and 66% respectively. 
                                                        This means that the first model ranks 91% of prospective renters correctly in terms of their conversion rate. 
                                                        This leads to:
                                                    </p>
                                                    
                                                    <p>
                                                        <span style="font-family:Montserrat; font-weight:700; font-size:150%; color:#fab300">Time savings</span> of 250 hours a year from leasing agents not having to manually go through each new prospect before deciding who to reach out to.<br>
                                                        <span style="font-family:Montserrat; font-weight:700; font-size:150%; color:#fab300">50% increase in successful rents</span> by leasing agents who are able to go after the most lucrative prospects from the get-go.<br>
                                                        <span style="font-family:Montserrat; font-weight:700; font-size:150%; color:#fab300">30% increase in Funnel's software functionality</span> thus enhancing its utility to leasing agents.
                                                    </p>
                                                <br>
                                                <br>
                                                <h3 style="text-align:left;">Data</h3>
                                                    <p>This project uses 60 million records with 400+ features that contain information about properties, prospective renters, and their interactions with leasing agents.</p>
                                                <br>
                                                <br>
                                                <h3 style="text-align:left;">Project Methodology</h3>
                                                <br>
                                                    <h4 style="text-align:left;">1. Data exploration: Data characteristics unique to this project</h4>
                                                        <p>Investigations reveal a special characteristic of this project's data:
                                                            Different subsets of the data are obtained at different points in time of the prospective renter's journey with a leasing agent. 
                                                            Specifically, more data is obtained as this journey progresses through the following stages:</p>
                                                            
                                                        <h5 style="text-align:left;">Stage 1: Prospect creation</h5>
                                                            <p>This stage occurs when a new prospective renter first expresses interest in a property. Here, the prospect has just entered into our database and we possess only basic information on them, such as:<br>
                                                                ▪ Prospective renter data: Age, Gender, ...<br>
                                                                ▪ Source data: How the prospective renter heard about the property<br>
                                                                ▪ Property data: Data on the property that the prospective renter expressed interest in<br>
                                                            </p>
                                                        
                                                        <h5 style="text-align:left;">Stage 2: Communication</h5>
                                                        <p>This stage occurs when the leasing agent and prospective renter subsequently start to exchange more and more communications. Over time, more data is accumulated on these interactions, such as:<br>
                                                            ▪ Medium of communications: Call, Email, Text, ...<br>
                                                            ▪ Number of communications per medium<br>
                                                            ▪ Reply speed of prospective renter<br>
                                                        </p> 

                                                        <h5 style="text-align:left;">Stage 3: Property Viewing</h5>
                                                        <p>This stage occurs when the prospective renter tours their property of interest. Here, we further obtain data on viewings, such as:<br>
                                                            ▪ Medium of viewing: In-person, Video call<br>
                                                            ▪ Speed of scheduling a viewing<br>
                                                            ▪ Outcome of viewing: Deposit put down, Deposit not put down<br>
                                                        </p>

                                                    <h4 style="text-align:left;">2. Make modelling decisions: Build 1 model per stage of prospective renter's journey</h4>
                                                        <p>Given the above, instead of creating one overarching model, we develop a three-model approach, to be respectively used at each stage of the prospective renter's journey, when we procure new data.
                                                            Each subsequent model uses more features than the last, since it uses the data of its preceding models, as well as the new data that comes in.</p>

                                                            <h5 style="text-align:center;">Model 1: Used on prospective renters at prospect creation stage</h5>
                                                            <h5 style="text-align:center;">Model 2: Used on prospective renters at communication stage</h5>
                                                            <h5 style="text-align:center;">Model 3: Used on prospective renters at property viewing stage</h5>
                                                        <br>
                                                        <p>Modelling in this manner has an important implication: We group prospective renters in each of the 3 stages of their journey.
                                                            For each stage, the corresponding model predicts conversion rate for only propective renters currently in that stage.
                                                            A higher predicted conversion rate in one stage would mean that that prospective renter is more likely to convert, out of all the other prospects in that same stage as him. 
                                                            The leasing agent should therefore prioritize following up on him, as opposed to other prospects in the same stage.</p>

                                                        <p>This runs opposite to the alternative modelling approach, which is to lump all prospective renters together, regardless of journey stage, 
                                                            and run an overarching model on all of them to obtain predicted conversion rates. This approach is far less useful, since it is clear
                                                            that the model would always predict a higher conversion rate to prospective renters in the later stages compared to former stages. 
                                                            Such information is unhelpful since one would be able to deduce this intuitively without the help of machine learning.</p>

                                                        <p>As such, the modelling decisions made in this project were carefully designed to provide the maximal utility possible to a leasing agent using Funnel's software,
                                                            in particular to provide him information that he would not have been able to deduce without the help of machine learning.</p>

                                                    <h4 style="text-align:left;">3. Feature selection: 400 to 50 features</h4>
                                                        <p>Since the dataset contains 400+ features, feature selection becomes imperative. This is because using a large number of features in modelling can lead to:<br>
                                                            ▪ Slow model training<br>
                                                            ▪ Large disk memory required for training<br>
                                                            ▪ Model performance degredation, in cases where many uninformative features are present<br>
                                                            We thus select features by looking at their Permutation Importance in a CatBoost model, resulting in a final subset of ~50 features.
                                                        </p>

                                                    <h4 style="text-align:left;">4. Feature engineering</h4>
                                                        <p>Hypothesizing that feature engineering would be key in model performance for this project,
                                                            we worked extensively with business stakeholders in Funnel to obtain domain knowledge behind each feature.
                                                            This allowed us to engineer useful features to provide greater signal to our model, such as:<br>
                                                            ▪ Creating time differences between when a prospective renter reaches out to a leasing agent and when the leasing agent replies<br>
                                                            ▪ Text analysis on conversations between a prospective renter and leasing agent: Length of text, text content, ... 
                                                        </p>

                                                    <h4 style="text-align:left;">5. Train &amp tune model hyperparameters using Bayesian Optimization: Logistic Regression, CatBoost, XGBoost</h4>
                                                        <p>
                                                            We benchmark the above suite of models to determine the best for our use-case.
                                                            To tune each model, I utilize a Bayesian Optimization search method with stratified k-fold cross validation in order to obtain the optimal hyperparameters in the most efficient manner.
                                                        </p>

                                                        <p>
                                                            Importantly, we choose to explore CatBoost since our use-case seemed ideal for it to shine:<br>
                                                            ▪ The final subset of features selected for modelling contains many categorical features.
                                                            CatBoost provides in-built categorical feature encoding, thus cutting down on significant time required for such encoding tasks prior to model building.
                                                            Read more about the CatBoost's hyperparameters for categorical feature encoding <a href="https://towardsdatascience.com/categorical-features-parameters-in-catboost-4ebd1326bee5#:~:text=The%20core%20idea%20behind%20CatBoost,only%20the%20objects%20that%20are" style="color: black;" target="_blank">here</a>! <br>
                                                            ▪ The dataset is extremely large with 60 million rows. This puts an extra strain on computational time for model building. 
                                                            CatBoost has been shown to be 8x faster than the state-of-the-art XGBoost.<br>
                                                            Read more about CatBoost: 
                                                                <a href="https://neptune.ai/blog/when-to-choose-catboost-over-xgboost-or-lightgbm" style="color: black;" target="_blank">CatBoost VS XGBoost VS LightGBM</a>,
                                                                <a href="https://hanishrohit.medium.com/whats-so-special-about-catboost-335d64d754ae" style="color: black;" target="_blank">What's so special about CatBoost?</a>
                                                        </p>
    
                                                        <p>
                                                            Our modelling results find CatBoost to indeed be the best performing model.
                                                        </p>

                                                    <h4 style="text-align:left;">6. Obtain SHAP feature importances</h4>
                                                        <p>Apart from building a highly performing machine learning model, another key aspect of this project is to investigate which variables contribute the most to a prospective renter's conversion rate, 
                                                            as well as the direction of its contribution (does a greater value for the feature increase or decrease conversion rate).
                                                        </p>

                                                        <p>Traditional impurity-based or permutation-based feature importance methods are unable to give this direction of contribution. As such, we utilize SHapley Additive exPlanations (SHAP) feature importances, which uses game theory in its computations.
                                                        </p>

                                                        <p>We obtain the following SHAP feature importance plot depicting the most important features:</p>
                                                        <center><img class="img-fluid" src="assets/images/portfolio/funnel/shap_feature_importance.png"></center>
                                                        

                                                        <p>Each point on the below plot is a Shapley value for a feature and a prospective renter. 
                                                            The Shapley value is the weighted average contribution of a feature value to the prediction. 
                                                            It relates to convertion rate as follows: Positive Shapley values indicate a conversion rate of more than 0.5 and negative Shapley values indicate a conversion rate of less than 0.5.
                                                            The position on the y-axis is determined by the feature and on the x-axis by the Shapley value. 
                                                            The color represents the value of the feature from low to high.  
                                                            Lastly, the features are ordered according to their importance.
                                                            This plot therefore answers the required questions we set out to investigate about feature importances.
                                                        </p>
                                                <br>
                                                <br>
                                                <h3 style="text-align:left;">One cool takeaway:<br>Using ROC-AUC to tailor metrics to business goals</h3>
                                                    <p>Machine learning models can be measured across a suite of metrics.
                                                        In order to make modelling decisions that push my project towards attaining its business goals, we specifically tailor the metric used in modelling (hyperparameter tuning, model selection) to these goals.
                                                        We choose the ROC-AUC (Area under the ROC curve) due to 3 key reasons:</p>

                                                    <h4 style="text-align:left;">1. Ranking</h4>
                                                        <p>Funnel aims to use the models built to rank prospective renters in terms of conversion rate, in order for leasing agents to prioritize them accordingly.
                                                            This immediately ties into ROC-AUC, since it measures model performance as the percentage of prospective renters that are correctly ranked by the model in terms of their conversion rate.
                                                        </p>

                                                    <h4 style="text-align:left;">2. Choice of curves: ROC VS PRC</h4>
                                                        <p>For binary classification tasks, the typical choice is between the Receiver Operating Curve (ROC) and Precision-Recall Curve (PRC).
                                                            The ROC curve plots True Positive Rate against False Positive Rate and takes into account model performance on both the True Positive and True Negative classes.
                                                            On the other hand, the PRC curve plots Precision against Recall, both of which are metrics that do not take into account the True Negative condition. As such, it takes into account model performance on only the True Positive class.
                                                        </p>

                                                        <p>In this project, it is equally important to catch a prospective renter who will rent (True Positive), as well as a prospective renter who will not rent (True Negative).
                                                            This is because the leasing agent can utilize different engagement strategies for each. 
                                                            Thus, we decide to use the ROC curve.  
                                                        </p>

                                                    <h4 style="text-align:left;">3. Properties of AUC</h4>
                                                        <p>A key property is that AUC is scale-invariant, ie. it measures how well conversion rates are ranked, rather than the absolute values of conversion rates.
                                                            Since the business goal is to rank prospective renters, we are less concerned with obtaining an exact conversion rate per prospect, as opposed to
                                                            obtaining a correct ranking for them. This presents another strong argument for using the AUC.</p>
                                                <br>
                                                <br>
                                                <h3 style="text-align:left;">Still curious?</h3>
                                                <p>My team made a presentation to Funnel, and you can check out our slides below.</p>
        
                                            <center>
                                                <button class="btn btn-primary btn-xl text-uppercase" type="button">
                                                    <a href="https://github.com/sheilateozy/Funnel_Predicting-Prospect-Conversion/blob/main/presentation_slides/presentation_slides.pdf" style="text-decoration: none; color: white;" target="_blank">Presentation Slides</a>
                                                </button>
        
                                                <button class="btn btn-primary btn-xl text-uppercase" data-bs-dismiss="modal" type="button">
                                                    <i class="fas fa-xmark me-1"></i>
                                                    Close Project
                                                </button>
                                            </center>
                                </div>
                            </div>
                        </div>
                    </div>
                </div>
            </div>
        </div>
        <!-- Page 6: Columbia -->
        <div class="portfolio-modal modal fade" id="portfolioModal6" tabindex="-1" role="dialog" aria-hidden="true">
            <div class="modal-dialog">
                <div class="modal-content">
                    <div class="close-modal" data-bs-dismiss="modal"><img src="assets/icons/close.svg" alt="Close modal" /></div>
                                <div class="modal-body">
                                    <!-- Project details-->
                                    <h2 class="text-uppercase">Predicting Avazu's click-through rates</h2>
                                    <h3 class="text-warning">CatBoost, XGBoost, LightGBM </h3>
                                    <p class="item-intro text-muted">
                                        Ensembled 12 CatBoost, XGBoost, LightGBM models using Stacking, with Elastic-Net Logistic Regression as meta-model.<br>
                                        Applied numerous categorical feature encoding methods on dataset with purely categorical features.<br>
                                        Extensive feature engineering to capture temporal nature of consumer clicks.<br>
                                        <b>Placed 4<sup>th</sup> out of 200 participants.</b>
                                    </p>


                                    <button class="btn btn-primary btn-xl text-uppercase" type="button">
                                        <a href="https://github.com/sheilateozy/ColumbiaCompetition_Predicting-Clickthrough-Rates" style="text-decoration: none; color: white;" target="_blank">View on GitHub</a>
                                    </button>
                                    <br>
                                    <br>
                                    <br>

                                    <div class="container">
                                        <div class="row justify-content-center">
                                            <div class="col-lg-8" style="text-align:justify;">
                                                <h3 style="text-align:left;">About</h3>
                                                    <p>I competed in a team of 3 in a Columbia Engineering competition open to Masters students.
                                                        This competition was adapted from Avazu's highly popular Kaggle competition in 2015, linked 
                                                        <a href="https://www.kaggle.com/c/avazu-ctr-prediction" style="color: black;" target="_blank">here</a>.
                                                    </p>

                                                    <p>Avazu is a programmatic advertising platform that uses machine learning to decide which mobile advertisements get pushed to which consumers.
                                                        Its aim is to maximize advertising effectiveness by ensuring the correct target group receieves the advertisements they are most interested in.
                                                    </p>
                                                        
                                                    <p>This competition is to predict consumer click-through rates on mobile advertisements, ie. whether a consumer clicks on an advertisement.</p>
                                                        
                                                    <p>In online advertising, click-through rate is a very important metric for evaluating ad performance and is used in sponsored search and real-time bidding.</p>
                                                <br>
                                                <br>
                                                <h3 style="text-align:left;">Data</h3>
                                                    <p>This competition uses 4 million rows with ~30 features that contain information about the consumer's mobile device, the mobile advertisement, the website on which the advertisement was encountered, etc.
                                                        Each row depicts a specific user on a specific mobile device, encountering a specific mobile advertisement, and the target of whether the user clicked on it.
                                                    </p>
                                                <br>
                                                <br>
                                                <h3 style="text-align:left;">Project Methodology</h3>
                                                <br>
                                                    <h4 style="text-align:left;">1. Obtain training and test sets via time split:<br>Due to time nature of data</h4>
                                                        <p>It is critical to observe that the data involves a certain time-series nature to it. 
                                                            The same consumer can encounter the mobile advertisements more than once at different points in time, some being the same advertisement as before and others being new advertisements.
                                                            The consumer's decision of whether to click on the advertisement, which is our target variable, can therefore be associated with the consumer's click history in the past. 
                                                            As such, the rows in our dataset are no longer independent of each other.</p>

                                                        <p>In this situation, the typical method of dividing data into training and test sets via a random split can therefore introduce data leakage from the train to the test set.
                                                            We therefore split based on a cut-off date and time instead. This cut-off is chosen such that the training set is ~80% and the test set is ~20% of the data.</p>
                                                        
                                                    <h4 style="text-align:left;">2. Feature engineering to identify unique consumers from device data</h4>
                                                        <br>
                                                        <h5 style="text-align:left;">2.1. <i>consumer</i></h5>
                                                            <p>There are multiple columns in the data that tracks mobile device information, 
                                                                such as <i>device_ip, device_id, device_model</i> etc. 
                                                                We define a unique consumer as one that has the same value for all of such columns. 
                                                            </p>

                                                            <p>Thus, we create a new column, <i>consumer</i>, which is the string concatenation of the values in these columns.
                                                                <i>consumer</i> therefore defines the unique consumers in our dataset.
                                                            </p>
                                                    
                                                    <h4 style="text-align:left;">3. Feature engineering to extract time nature of data</h4>
                                                        <br>
                                                        <h5 style="text-align:left;"><i>3.1. click_history</i></h5>
                                                            <p>This tracks the click history made by the same user up to the current point in time, depicted as a string of previous click history.</p>
                                                        
                                                            <p>
                                                            For example, assume there are 2 users, A and B, and they appear in the toy dataset below, with rows sorted chronologically. 
                                                            Note that <i>target</i> refers to whether or not the user clicked on the mobile advertisement they encountered. 
                                                            Thus, <i>click_history</i> is engineered from <i>target</i>, as follows. 
                                                            </p>

                                                            <center>
                                                                <table style="border:1px dotted black;">
                                                                    <tr>
                                                                        <th style="border:1px dotted black;"><i>&nbsp; time &nbsp;</i></th>
                                                                        <th style="border:1px dotted black;"><i>&nbsp; user &nbsp;</i></th>
                                                                        <th style="border:1px dotted black;"><i>&nbsp; target &nbsp;</i></th>
                                                                        <th style="border:1px dotted black;"><i>&nbsp; click_history &nbsp;</i></th>
                                                                    </tr>
                                                                    <tr>
                                                                        <td style="border:1px dotted black; text-align: center; vertical-align: middle;">0900</td>
                                                                        <td style="border:1px dotted black; text-align: center; vertical-align: middle;">A</td>
                                                                        <td style="border:1px dotted black; text-align: center; vertical-align: middle;">1</td>
                                                                        <td style="border:1px dotted black; text-align: center; vertical-align: middle;">NA</td>
                                                                    </tr>
                                                                    <tr>
                                                                        <td style="border:1px dotted black; text-align: center; vertical-align: middle;">0901</td>
                                                                        <td style="border:1px dotted black; text-align: center; vertical-align: middle;">A</td>
                                                                        <td style="border:1px dotted black; text-align: center; vertical-align: middle;">0</td>
                                                                        <td style="border:1px dotted black; text-align: center; vertical-align: middle;">"1"</td>
                                                                    </tr>
                                                                    <tr>
                                                                        <td style="border:1px dotted black; text-align: center; vertical-align: middle;">0902</td>
                                                                        <td style="border:1px dotted black; text-align: center; vertical-align: middle;" >B</td>
                                                                        <td style="border:1px dotted black; text-align: center; vertical-align: middle;">0</td>
                                                                        <td style="border:1px dotted black; text-align: center; vertical-align: middle;">NA</td>
                                                                    </tr>
                                                                    <tr>
                                                                        <td style="border:1px dotted black; text-align: center; vertical-align: middle;">0903</td>
                                                                        <td style="border:1px dotted black; text-align: center; vertical-align: middle;">A</td>
                                                                        <td style="border:1px dotted black; text-align: center; vertical-align: middle;">1</td>
                                                                        <td style="border:1px dotted black; text-align: center; vertical-align: middle;">"10"</td>
                                                                    </tr>
                                                                    <tr>
                                                                        <td style="border:1px dotted black; text-align: center; vertical-align: middle;">0904</td>
                                                                        <td style="border:1px dotted black; text-align: center; vertical-align: middle;">B</td>
                                                                        <td style="border:1px dotted black; text-align: center; vertical-align: middle;">0</td>
                                                                        <td style="border:1px dotted black; text-align: center; vertical-align: middle;">"0"</td>
                                                                    </tr>
                                                                    <tr>
                                                                        <td style="border:1px dotted black; text-align: center; vertical-align: middle;">0905</td>
                                                                        <td style="border:1px dotted black; text-align: center; vertical-align: middle;">A</td>
                                                                        <td style="border:1px dotted black; text-align: center; vertical-align: middle;">0</td>
                                                                        <td style="border:1px dotted black; text-align: center; vertical-align: middle;">"101"</td>
                                                                    </tr>
                                                                    <tr>
                                                                        <td style="border:1px dotted black; text-align: center; vertical-align: middle;">0906</td>
                                                                        <td style="border:1px dotted black; text-align: center; vertical-align: middle;">B</td>
                                                                        <td style="border:1px dotted black; text-align: center; vertical-align: middle;">1</td>
                                                                        <td style="border:1px dotted black; text-align: center; vertical-align: middle;">"00"</td>
                                                                    </tr>
                                                                </table>
                                                            </center>
                                                        <br>
                                                        <br>
                                                        <h5 style="text-align:left;">3.2. <i>hour_of_day, day_of_week</i></h5>
                                                            <p>We converted the original <i>time</i> column into 2 separate columns to better extract signal for our model.</p>
                                                        
                                                        <h5 style="text-align:left;">3.3. <i>device_id_count</i></h5>
                                                            <p>This depicts the total number of mobile advertisements that each mobile device encounters up to the current point in time.
                                                                We hypothesize that this influences whether or not a user using that mobile device will click on the next advertisement they encounter.
                                                            </p>
                                                        
                                                        <h5 style="text-align:left;">3.4. <i>hour_user_count</i></h5>
                                                            <p>This depicts the total number of mobile advertisements that each user encounters up to the current point in time, in each hour of day.
                                                                Again, we make the same hypothesis as above.
                                                            </p>
                                                    <br>
                                                    <h4 style="text-align:left;">4. Feature cleaning of rare values</h4>
                                                        <p>For catgorical features, we remove rare feature values, defined as levels of the feature that only appear once.
                                                            We group such levels together into a "Rare" category.
                                                        </p>
                                                    <br>
                                                    <h4 style="text-align:left;">5. Encode categorical features:<br>Using Hash Encoding, Ordered Target Encoding, Ordinal Encoding</h4>
                                                        <p>The dataset consists of purely categorical features.
                                                            Thus, we explore multiple encoding methods to determine the best methods per feature type.
                                                        </p>

                                                        <h5 style="text-align:left;">5.1. Hash Encoding: For features with high cardinality</h5>
                                                            <p>
                                                                Hash Encoding reduces the number of new features created from the encoding to a lower dimension compared to that of the original feature. 
                                                                This is useful for features with high cardinality.
                                                            </p>

                                                        <h5 style="text-align:left;">5.2. Ordinal Encoding: For features with intuitive ordering</h5>
                                                            <p>
                                                                For categorical features in which there exists an intuitive ordering among feature levels, 
                                                                we use Ordinal Encoding to capture this ordering for the model.
                                                            </p>

                                                        <h5 style="text-align:left;">5.3. Ordered Target Encoding: For remaining features</h5>
                                                            <p>
                                                                Target Encoding is the substitution of a category with the mean target value for that category.
                                                                However, this introduces target leakage since the target-encoded value for an observation includes the effect of that observation's target value,
                                                                thus this encoded feature value for the observation has the target baked inside it. 
                                                            </p>

                                                            <p>
                                                                Ordered Target Encoding builds upon traditional Target Encoding by alleviating this issue.
                                                                It is an encoding method proposed by the developers of CatBoost, and is regarded to be part of the reason why CatBoost performs so well on categorical data.
                                                                It thus also goes by the name CatBoost Target Encoding.
                                                            </p>

                                                            <p>
                                                                Ordered Target Encoding works as follows:<br>
                                                                1. Randomly permute the rows of the training set.<br>
                                                                2. The target-encoded value for a category in the <i>i<sup>th</sup></i> observation is computed as the mean target value of that category in all previous observations placed before it in the dataset.
                                                                    This is akin to taking a running average down the dataset. <br>
                                                                3. This creates a problem: For the first few rows in the dataset, target-encoded values have high variance since they were calculated using very few observations. 
                                                                    However, as we move down the dataset, the target-encoded values become increasingly stable.
                                                                    This means the initial part of the dataset has weak estimates compared to the rest of it,
                                                                    and this initial part of the dataset was randomly chosen from Step 1.<br><br>

                                                                    CatBoost handles this problem by conducting more than 1 random permutations of the dataset and repeating all the above steps for each permutation.
                                                                    Assuming CatBoost conducts <i>n</i> random permutations, it then obtains <i>n</i> different target-encoded values for each categorical feature in each observation.
                                                                    At each step of gradient boosting, CatBoost randomly chooses one of these <n>target-encoded values</n> to be used.
                                                                    This alleviates the problem outlined.
                                                            </p>
                                                    <br>
                                                    <h4 style="text-align:left;">6. Train &amp tune model hyperparameters using Bayesian Optimization: CatBoost, XGBoost, LightGBM</h4>
                                                        <p>
                                                            To tune each model, I utilize a Bayesian Optimization search method with stratified k-fold cross validation in order to obtain the optimal hyperparameters in the most efficient manner.
                                                        </p>
                                                    
                                                        <p>The rationale for choosing these specific models are:<br>
                                                            ▪ XGBoost: Strong history of high performance on Kaggle competitions<br>
                                                            ▪ LightGBM: A variant of XGBoost with faster computational speed, useful given our large dataset<br>
                                                            ▪ CatBoost: A variant of XGBoost with in-built categorical feature encoding, useful given the many categorical features in our dataset
                                                        </p>

                                                        <p>
                                                            In total, 12 CatBoost, XGBoost, and LightGBM models were trained and tuned.
                                                        </p>
                                                    <br>
                                                    <h4 style="text-align:left;">7. Ensemble models:<br>Using Stacking, with Elastic Net Logistic Regression as meta-model</h4>
                                                        <p>
                                                            1. Feed the predictions of each of the 12 models as inputs to an Elastic Net Logistic Regression.<br>
                                                            2. Train and tune Elastic-Net Logistic Regression using Bayesian Optimization search method with stratified k-fold cross validation.<br>
                                                            3. The Elastic-Net Logistic Regression essentially combines the predictions of the 12 models via a weighted average. 
                                                            The prediction of the Elastic-Net Logistic Regression is therefore our final prediction.<br>
                                                        </p>

                                                        <h5 style="text-align:left;">7.1. Why Logistic Regression?</h5>
                                                            <p>We choose Logistic Regression as our meta-model for Stacking since this is a classification task, 
                                                                thus we require the output of our meta-model to be constrained between 0 and 1.
                                                            </p>

                                                        <h5 style="text-align:left;">7.2. Why Elastic Net regularization?</h5>
                                                            <p>
                                                                Since the inputs to the Logistic Regression are the predictions of each of the 12 models,
                                                                using an Elastic Net regulatization on the regression essentially tells the regression to figure out how to best combine the 12 models' predictions (which predictions to put greater or less weight on)
                                                                to optimally fit the target. 
                                                            </p>

                                                            <p>This allows us to ensemble the 12 models' predictions in a more systematic manner, as opposed to taking a simple average of the 12 predictions.</p>

                                                    <h4 style="text-align:left;">8. Re-run on full data</h4>
                                                        <p>
                                                            In all the above steps, we had divided the competition data into a training set (80%) and test set (20%).
                                                            Now, armed with the best hyperparamteters found for each model as well as the meta-model, we re-run all of the above on the full competition training dataset to maximize training data before using the models to obtain a final prediction for the competition test set.
                                                        </p>
                                            <center>
                                                <button class="btn btn-primary btn-xl text-uppercase" data-bs-dismiss="modal" type="button">
                                                    <i class="fas fa-xmark me-1"></i>
                                                    Close Project
                                                </button>
                                            </center>
                                </div>
                            </div>
                        </div>
                    </div>
                </div>
            </div>
        </div>









        <!-- Bootstrap core JS-->
        <script src="https://cdn.jsdelivr.net/npm/bootstrap@5.1.3/dist/js/bootstrap.bundle.min.js"></script>
        <!-- Core theme JS-->
        <script src="js/scripts.js"></script>
        <!-- * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * *-->
        <!-- * *                               SB Forms JS                               * *-->
        <!-- * * Activate your form at https://startbootstrap.com/solution/contact-forms * *-->
        <!-- * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * *-->
        <script src="https://cdn.startbootstrap.com/sb-forms-latest.js"></script>
    </body>
</html>
